{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddd81073-43f5-49c2-99d5-9b0b0bff2040",
   "metadata": {},
   "source": [
    "# UKB RAP Dataset Filtering\n",
    "\n",
    "This notebook outlines the filtering of UKB for individuals IDs based on filtering criteria.\n",
    "It provides a more detailed, fine-scale filtering system than the one implemented in the Cohort Explorer.\n",
    "\n",
    "It filters for individuals with disease status using\n",
    "1. Hospital diagnoses (ICD10/9)\n",
    "2. Death cause record mention of disease X\n",
    "3. Self-reported mention of disease X\n",
    "4. First occurrence of disease X (if suitable)\n",
    "\n",
    "It can also filter for individuals who have a certain operation record as well.\n",
    "\n",
    "Code as per: https://github.com/dnanexus/OpenBio/blob/master/UKB_notebooks/ukb-rap-pheno-basic.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7793694d-4c6f-42dc-a6c4-46a5513d5c6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import pyspark\n",
    "import dxpy\n",
    "import dxdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cbe4a30-88c6-48ab-a631-655b22c8a3df",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.41.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dxdata.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c643e7a4-3b4f-4ca5-b0c3-5e2f855fb5c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Spark initialization (Done only once; do not rerun this cell unless you select Kernel -> Restart kernel).\n",
    "sc = pyspark.SparkContext()\n",
    "spark = pyspark.sql.SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2bb0ea8-3603-4342-afc9-937d6d8144fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Automatically discover dispensed database name and dataset id\n",
    "dispensed_database = dxpy.find_one_data_object(\n",
    "    classname='database', \n",
    "    name='app*', \n",
    "    folder='/', \n",
    "    name_mode='glob', \n",
    "    describe=True)\n",
    "dispensed_database_name = dispensed_database['describe']['name']\n",
    "\n",
    "dispensed_dataset = dxpy.find_one_data_object(\n",
    "    typename='Dataset', \n",
    "    name='app*.dataset', \n",
    "    folder='/', \n",
    "    name_mode='glob')\n",
    "dispensed_dataset_id = dispensed_dataset['id']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4c3957-e1c4-4b02-a998-242d9ae817df",
   "metadata": {},
   "source": [
    "## Access the Dataset File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83241c81-f0d4-458d-bcfa-158ec04a40d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = dxdata.load_dataset(id=dispensed_dataset_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c387658b-b78c-46c0-86cc-3948de33b701",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Entity \"participant\">,\n",
       " <Entity \"covid19_result_england\">,\n",
       " <Entity \"covid19_result_scotland\">,\n",
       " <Entity \"covid19_result_wales\">,\n",
       " <Entity \"gp_clinical\">,\n",
       " <Entity \"gp_scripts\">,\n",
       " <Entity \"gp_registrations\">,\n",
       " <Entity \"hesin\">,\n",
       " <Entity \"hesin_diag\">,\n",
       " <Entity \"hesin_oper\">,\n",
       " <Entity \"hesin_critical\">,\n",
       " <Entity \"hesin_maternity\">,\n",
       " <Entity \"hesin_delivery\">,\n",
       " <Entity \"hesin_psych\">,\n",
       " <Entity \"death\">,\n",
       " <Entity \"death_cause\">,\n",
       " <Entity \"omop_death\">,\n",
       " <Entity \"omop_device_exposure\">,\n",
       " <Entity \"omop_note\">,\n",
       " <Entity \"omop_observation\">,\n",
       " <Entity \"omop_drug_exposure\">,\n",
       " <Entity \"omop_observation_period\">,\n",
       " <Entity \"omop_person\">,\n",
       " <Entity \"omop_procedure_occurrence\">,\n",
       " <Entity \"omop_specimen\">,\n",
       " <Entity \"omop_visit_detail\">,\n",
       " <Entity \"omop_visit_occurrence\">,\n",
       " <Entity \"omop_dose_era\">,\n",
       " <Entity \"omop_drug_era\">,\n",
       " <Entity \"omop_condition_era\">,\n",
       " <Entity \"omop_condition_occurrence\">,\n",
       " <Entity \"omop_measurement\">,\n",
       " <Entity \"olink_instance_0\">,\n",
       " <Entity \"olink_instance_2\">,\n",
       " <Entity \"olink_instance_3\">]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Print out the entities within that dataset (i.e subtables including main entity = 'participant')\n",
    "dataset.entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c82ee9d-e5cb-4c30-8e61-840f799ddd6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "participant = dataset['participant']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a99ddf2f-3259-42f1-b734-64b505e563dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Returns all field objects for a given UKB showcase field id\n",
    "\n",
    "def fields_for_id(field_id):\n",
    "    from distutils.version import LooseVersion\n",
    "    field_id = str(field_id)\n",
    "    fields = participant.find_fields(name_regex=r'^p{}(_i\\d+)?(_a\\d+)?$'.format(field_id))\n",
    "    return sorted(fields, key=lambda f: LooseVersion(f.name))\n",
    "\n",
    "# Returns all field names for a given UKB showcase field id\n",
    "\n",
    "def field_names_for_id(field_id):\n",
    "    return [f.name for f in fields_for_id(field_id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2ae7e63c-815c-4365-be64-8899c49dc60d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_82/339987039.py:7: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  return sorted(fields, key=lambda f: LooseVersion(f.name))\n"
     ]
    }
   ],
   "source": [
    "#This provides all the field-names for all the instances/arrays of a single field-name.\n",
    "\n",
    "#Hospital ICD10 codes:\n",
    "field_names_for_id(41270)\n",
    "\n",
    "#Hospital ICD9 codes:\n",
    "field_names_for_id(41271)\n",
    "\n",
    "#Death register \n",
    "field_names_for_id(40001)\n",
    "field_names_for_id(40002)\n",
    "\n",
    "#Verbal interview non-cancer illness codes\n",
    "non_cancer_illness_fields = sum([field_names_for_id(20002)],[]) \n",
    "\n",
    "#First occurrence of HF (I50) and T2D (E11)\n",
    "first_hf_occurrence= field_names_for_id(131354)\n",
    "first_t2d_occurrence= field_names_for_id(130708)\n",
    "\n",
    "#Verbal interview self-reported operation codes\n",
    "operations_selfreport = sum([field_names_for_id(20004)],[]) \n",
    "\n",
    "#Verbal interview self-reported operation date codes\n",
    "operations_selfreport_dates = sum([field_names_for_id(20010)],[]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d672b2a4-dac0-40ef-950a-cac5e0673af6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#This outlines the field names of the participants entity that I will be using..\n",
    "participant_field_names_of_interest = ['eid', 'p41270','p41271'] + non_cancer_illness_fields + operations_selfreport + operations_selfreport_dates + first_hf_occurrence + first_t2d_occurrence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f74113-ca4e-4a51-92cd-a02ed8c21824",
   "metadata": {},
   "source": [
    "## Extraction of Chosen Fields from Participant Entity -> Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f56e2b6a-ec1f-4d28-bc09-4e957b20bea9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "main_df = participant.retrieve_fields(names=participant_field_names_of_interest, engine=dxdata.connect(), coding_values='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3f4f4f6-b675-47c9-90de-ceba31509570",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Show the head\n",
    "#main_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72087a44-22b2-4b11-8200-69360c097f71",
   "metadata": {},
   "source": [
    "## Filtering for Participant IDs in the Main Participant Entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "82b53432-3f73-4919-9ede-2ad8b6a92a36",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44437"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "#Change your ICD-10 code for filtering here\n",
    "\n",
    "#For HF\n",
    "# main_icd10 = main_df.filter(\n",
    "#     (F.array_contains(main_df.p41270,'I50.0 Congestive heart failure'))|\n",
    "#     (F.array_contains(main_df.p41270,'I50.1 Left ventricular failure'))|\n",
    "#     (F.array_contains(main_df.p41270,'I50.9 Heart failure, unspecified')) \n",
    "# )\n",
    "\n",
    "#For hypertensive heart disease\n",
    "# main_icd10 = main_df.filter(\n",
    "#     (F.array_contains(main_df.p41270,'I11.0 Hypertensive heart disease with (congestive) heart failure'))|\n",
    "#     (F.array_contains(main_df.p41270,'I11.9 Hypertensive heart disease without (congestive) heart failure'))\n",
    "# )\n",
    "\n",
    "#For HCM\n",
    "# main_icd10 = main_df.filter(\n",
    "#     (F.array_contains(main_df.p41270,'I42.1 Obstructive hypertrophic cardiomyopathy'))|\n",
    "#     (F.array_contains(main_df.p41270,'I42.2 Other hypertrophic cardiomyopathy'))\n",
    "# )\n",
    "\n",
    "#For T2D\n",
    "main_icd10 = main_df.filter(\n",
    "    (F.array_contains(main_df.p41270,'E11.0 With coma'))|\n",
    "    (F.array_contains(main_df.p41270,'E11.1 With ketoacidosis'))|\n",
    "    (F.array_contains(main_df.p41270,'E11.2 With renal complications'))|\n",
    "    (F.array_contains(main_df.p41270,'E11.3 With ophthalmic complications'))|\n",
    "    (F.array_contains(main_df.p41270,'E11.4 With neurological complications'))|\n",
    "    (F.array_contains(main_df.p41270,'E11.5 With peripheral circulatory complications'))|\n",
    "    (F.array_contains(main_df.p41270,'E11.6 With other specified complications'))|\n",
    "    (F.array_contains(main_df.p41270,'E11.7 With multiple complications'))|\n",
    "    (F.array_contains(main_df.p41270,'E11.8 With unspecified complications'))|\n",
    "    (F.array_contains(main_df.p41270,'E11.9 Without complications'))\n",
    ")\n",
    "\n",
    "main_icd10.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f540db64-ca51-4380-8276-6e28a1aacf7b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Filter within ICD-9 summary diagnoses\n",
    "\n",
    "#For HF\n",
    "# main_icd9 = main_df.filter(\n",
    "#     (F.array_contains(main_df.p41271,'4280 Congestive heart failure'))|\n",
    "#     (F.array_contains(main_df.p41271,'4281 Left heart failure'))|\n",
    "#     (F.array_contains(main_df.p41271,'4289 Heart failure, unspecified'))\n",
    "# )\n",
    "\n",
    "#For HCM\n",
    "# main_icd9 = main_df.filter(\n",
    "#     (F.array_contains(main_df.p41271,'4251 Hypertrophic obstructive cardiomyopathy'))\n",
    "# )\n",
    "\n",
    "#For T2D\n",
    "main_icd9 = main_df.filter(\n",
    "    (F.array_contains(main_df.p41271,'25000 Diabetes mellitus without mention of complication (adult-onset type)'))|\n",
    "    (F.array_contains(main_df.p41271,'25010 Diabetes with ketoacidosis (adult-onset type)'))\n",
    ")\n",
    "\n",
    "main_icd9.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a357bc7b-4e6f-4ee1-9d66-06950fe37ace",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5164"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Filter within Interview/Self-Reported\n",
    "\n",
    "from functools import reduce\n",
    "#Also filter for individuals with  via verbal interview\n",
    "# Step 1: Identify columns starting with 'p2002'\n",
    "p2002_cols = [col for col in main_df.columns if col.startswith('p20002')]\n",
    "\n",
    "# Step 2: Create condition for each column and change here too if you want to filter for other diseases\n",
    "\n",
    "#For HF\n",
    "# conditions = [F.col(col).contains('heart failure') for col in p2002_cols]\n",
    "\n",
    "#For HCM\n",
    "# conditions = [F.col(col).contains('hypertrophic cardiomyopathy (hcm / hocm)') for col in p2002_cols]\n",
    "\n",
    "#For T2D\n",
    "conditions = [F.col(col).contains('type 2 diabetes') for col in p2002_cols]\n",
    "\n",
    "\n",
    "# Step 3: Combine all conditions\n",
    "all_conditions = reduce(lambda a, b: a | b, conditions)\n",
    "\n",
    "# Step 4: Apply the filter\n",
    "interview = main_df.filter(all_conditions)\n",
    "interview.count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46cb01f-cc7c-42d0-940a-1cb052029942",
   "metadata": {},
   "source": [
    "# Filtering for Participant IDs using First Occurrence Data Fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "625885f9-1200-4245-86d4-717897ad3936",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47076"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#For HF\n",
    "# main_firstocc = main_df.filter(\n",
    "#     (~ F.isnull(main_df.p131354))\n",
    "# )\n",
    "\n",
    "#For T2D\n",
    "main_firstocc = main_df.filter(\n",
    "    (~ F.isnull(main_df.p130708))\n",
    ")\n",
    "\n",
    "main_firstocc.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c433ccf-be58-40a8-b9d1-f6e3f3775cac",
   "metadata": {},
   "source": [
    "## Filter for Participant IDs via Death Cause in Registry - ARCHIVED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5b152d-e634-4f85-be60-c6f270d239f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter within primary and secondary causes of death via ICD-10 codes.\n",
    "#This is absent from the participant table so using raw death cause record data instead.\n",
    "\n",
    "#Change your ICD-10 code for filtering here\n",
    "\n",
    "#For HF\n",
    "# death1_icd10 = main_df.filter(\n",
    "#     (F.array_contains(main_df.p40001,'I50.0 Congestive heart failure'))|\n",
    "#     (F.array_contains(main_df.p40001,'I50.1 Left ventricular failure'))|\n",
    "#     (F.array_contains(main_df.p40001,'I50.9 Heart failure, unspecified')) \n",
    "# )\n",
    "\n",
    "#For hypertensive heart disease\n",
    "# death1_icd10 = main_df.filter(\n",
    "#     (F.array_contains(main_df.p40001,'I11.0 Hypertensive heart disease with (congestive) heart failure'))|\n",
    "#     (F.array_contains(main_df.p40001,'I11.9 Hypertensive heart disease without (congestive) heart failure'))\n",
    "# )\n",
    "\n",
    "#For HCM\n",
    "# death1_icd10 = main_df.filter(\n",
    "#     (F.array_contains(main_df.p40001,'I42.1 Obstructive hypertrophic cardiomyopathy'))|\n",
    "#     (F.array_contains(main_df.p40001,'I42.2 Other hypertrophic cardiomyopathy'))\n",
    "# )\n",
    "\n",
    "# death1_icd10.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01570885-a978-413a-aef3-2408631adf43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For HF\n",
    "# death2_icd10 = main_df.filter(\n",
    "#     (F.array_contains(main_df.p40002,'I50.0 Congestive heart failure'))|\n",
    "#     (F.array_contains(main_df.p40002,'I50.1 Left ventricular failure'))|\n",
    "#     (F.array_contains(main_df.p40002,'I50.9 Heart failure, unspecified')) \n",
    "# )\n",
    "\n",
    "#For hypertensive heart disease\n",
    "# death2_icd10 = main_df.filter(\n",
    "#     (F.array_contains(main_df.p40002,'I11.0 Hypertensive heart disease with (congestive) heart failure'))|\n",
    "#     (F.array_contains(main_df.p40002,'I11.9 Hypertensive heart disease without (congestive) heart failure'))\n",
    "# )\n",
    "\n",
    "#For HCM\n",
    "# death2_icd10 = main_df.filter(\n",
    "#     (F.array_contains(main_df.p40002,'I42.1 Obstructive hypertrophic cardiomyopathy'))|\n",
    "#     (F.array_contains(main_df.p40002,'I42.2 Other hypertrophic cardiomyopathy'))\n",
    "# )\n",
    "\n",
    "# death2_icd10.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a2512f-ffa8-43b9-9ba3-ba6218d2ebc0",
   "metadata": {},
   "source": [
    "## Filtering for Participant IDs in the Disease Cause Record Entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "406b382b-243f-433f-9586-82aaff29b948",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#I also extract the death cause record data for filtering\n",
    "death_cause_record = dataset['death_cause']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "99e1e8ff-d5de-4154-bdbe-458e46d74d48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Returns all field objects for a given title keyword\n",
    "\n",
    "def fields_by_title_keyword(keyword):\n",
    "    from distutils.version import LooseVersion\n",
    "    fields = list(death_cause_record.find_fields(lambda f: keyword.lower() in f.title.lower()))\n",
    "    return sorted(fields, key=lambda f: LooseVersion(f.name))\n",
    "\n",
    "# Returns all field names for a given title keyword\n",
    "\n",
    "def field_names_by_title_keyword(keyword):\n",
    "    return [f.name for f in fields_by_title_keyword(keyword)]\n",
    "\n",
    "# Returns all field titles for a given title keyword\n",
    "\n",
    "def field_titles_by_title_keyword(keyword):\n",
    "    return [f.title for f in fields_by_title_keyword(keyword)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8add65ab-a914-4969-8422-ef10b26bc073",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "death_df = death_cause_record.retrieve_fields(names=['eid','cause_icd10'], engine=dxdata.connect(), coding_values='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f0ff10cb-4f87-4ede-b2e8-a01f02278e8e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2382"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#For HF\n",
    "# death_df_icd10 = death_df.filter(\n",
    "#     (death_df.cause_icd10.contains('I50.0 Congestive heart failure'))|\n",
    "#     (death_df.cause_icd10.contains('I50.1 Left ventricular failure'))|\n",
    "#     (death_df.cause_icd10.contains('I50.9 Heart failure, unspecified'))\n",
    "# )\n",
    "\n",
    "#For hypertensive heart disease\n",
    "# death_df_icd10 = death_df.filter(\n",
    "#     (death_df.cause_icd10.contains('I11.0 Hypertensive heart disease with (congestive) heart failure'))|\n",
    "#     (death_df.cause_icd10.contains('I11.9 Hypertensive heart disease without (congestive) heart failure'))\n",
    "# )\n",
    "\n",
    "#For HCM\n",
    "# death_df_icd10 = death_df.filter(\n",
    "#     (death_df.cause_icd10.contains('I42.1 Obstructive hypertrophic cardiomyopathy'))|\n",
    "#     (death_df.cause_icd10.contains('I42.2 Other hypertrophic cardiomyopathy'))\n",
    "# )\n",
    "\n",
    "#For T2D\n",
    "death_df_icd10 = death_df.filter(\n",
    "    (death_df.cause_icd10.contains('E11.0 With coma'))|\n",
    "    (death_df.cause_icd10.contains('E11.1 With ketoacidosis'))|\n",
    "    (death_df.cause_icd10.contains('E11.2 With renal complications'))|\n",
    "    (death_df.cause_icd10.contains('E11.3 With ophthalmic complications'))|\n",
    "    (death_df.cause_icd10.contains('E11.4 With neurological complications'))|\n",
    "    (death_df.cause_icd10.contains('E11.5 With peripheral circulatory complications'))|\n",
    "    (death_df.cause_icd10.contains('E11.6 With other specified complications'))|\n",
    "    (death_df.cause_icd10.contains('E11.7 With multiple complications'))|\n",
    "    (death_df.cause_icd10.contains('E11.8 With unspecified complications'))|\n",
    "    (death_df.cause_icd10.contains('E11.9 Without complications'))\n",
    ")\n",
    "\n",
    "death_df_icd10.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef816f0-b57f-4c58-9ae4-8e009d89a16b",
   "metadata": {},
   "source": [
    "## Miscellaneous Sources e.g CMR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b9d42b82-23ab-4164-9c85-f1073818e273",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Field name(s) not found: ['p24140']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#For HCM\u001b[39;00m\n\u001b[1;32m      2\u001b[0m misc_names_of_interest \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meid\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mp24140\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 3\u001b[0m misc_df \u001b[38;5;241m=\u001b[39m \u001b[43mparticipant\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve_fields\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmisc_names_of_interest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdxdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoding_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreplace\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#For HCM\u001b[39;00m\n\u001b[1;32m      6\u001b[0m misc_pass \u001b[38;5;241m=\u001b[39m misc_df\u001b[38;5;241m.\u001b[39mfilter(\n\u001b[1;32m      7\u001b[0m     (misc_df\u001b[38;5;241m.\u001b[39mp24140 \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m15\u001b[39m)\n\u001b[1;32m      8\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/dxdata/dataset/dataset.py:2018\u001b[0m, in \u001b[0;36mEntity.retrieve_fields\u001b[0;34m(self, engine, fields, names, titles, filter_sql, coding_values, limit, column_aliases, array_as_string)\u001b[0m\n\u001b[1;32m   1984\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mretrieve_fields\u001b[39m(\u001b[38;5;28mself\u001b[39m, engine, fields\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, names\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, titles\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1985\u001b[0m                     filter_sql\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, coding_values\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m, limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1986\u001b[0m                     column_aliases\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, array_as_string\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1988\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build and execute SQL to retrieve fields' database column values.\u001b[39;00m\n\u001b[1;32m   1989\u001b[0m \n\u001b[1;32m   1990\u001b[0m \u001b[38;5;124;03m        Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2015\u001b[0m \u001b[38;5;124;03m        Returns: Spark dataframe\u001b[39;00m\n\u001b[1;32m   2016\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2018\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretrieve_fields\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mentity_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2019\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mfields\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfields\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtitles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtitles\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilter_sql\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilter_sql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2020\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mcoding_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoding_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn_aliases\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumn_aliases\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2021\u001b[0m \u001b[43m                           \u001b[49m\u001b[43marray_as_string\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43marray_as_string\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/dxdata/dataset/dataset.py:95\u001b[0m, in \u001b[0;36mretrieve_fields\u001b[0;34m(engine, dataset, entity_name, fields, names, titles, field_alias_map, filter_sql, coding_values, limit, column_aliases, array_as_string)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnly use one of: fields, names, titles\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m field_alias_map \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fields:\n\u001b[0;32m---> 95\u001b[0m     fields \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_find_fields\u001b[49m\u001b[43m(\u001b[49m\u001b[43mentity_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtitles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtitles\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m primary_key_field \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mprimary_entity[dataset\u001b[38;5;241m.\u001b[39mprimary_entity\u001b[38;5;241m.\u001b[39mprimary_key]\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m entity_name:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/dxdata/dataset/dataset.py:960\u001b[0m, in \u001b[0;36mDataset._find_fields\u001b[0;34m(self, entity_name, names, titles)\u001b[0m\n\u001b[1;32m    958\u001b[0m     missing_names \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(names) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(name_lookup)\n\u001b[1;32m    959\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m missing_names:\n\u001b[0;32m--> 960\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mField name(s) not found: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    961\u001b[0m                          \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28msorted\u001b[39m(missing_names)))\n\u001b[1;32m    962\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    963\u001b[0m     \u001b[38;5;66;03m# We return all fields that match titles\u001b[39;00m\n\u001b[1;32m    964\u001b[0m     fields \u001b[38;5;241m=\u001b[39m [fld \u001b[38;5;28;01mfor\u001b[39;00m fld \u001b[38;5;129;01min\u001b[39;00m found_fields]\n",
      "\u001b[0;31mValueError\u001b[0m: Field name(s) not found: ['p24140']"
     ]
    }
   ],
   "source": [
    "#For HCM\n",
    "# misc_names_of_interest = ['eid', 'p24140']\n",
    "# misc_df = participant.retrieve_fields(names=misc_names_of_interest, engine=dxdata.connect(), coding_values='replace')\n",
    "\n",
    "# #For HCM\n",
    "# misc_pass = misc_df.filter(\n",
    "#     (misc_df.p24140 >= 15)\n",
    "# )\n",
    "\n",
    "# misc_pass.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79121bca-cacf-4345-ae2b-77bd1467aed8",
   "metadata": {},
   "source": [
    "## Aggregation of Different Data Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "513d2032-f173-45e7-8bf5-33c3fa7180a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "## Filter for all overlapping EID in the three filtered Spark dataframe\n",
    "## Convert to Pandas DataFrame and then overlap\n",
    "\n",
    "\n",
    "main_icd10_pd = main_icd10.toPandas()\n",
    "main_icd9_pd = main_icd9.toPandas()\n",
    "firstocc_pd= main_firstocc.toPandas()\n",
    "death_df_icd10_pd = death_df_icd10.toPandas()\n",
    "interview_pd = interview.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9baf56e0-d1f5-4fee-8228-48491b08181a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "overlap_eid = pd.concat([death_df_icd10_pd['eid'],\n",
    "                        main_icd10_pd['eid'],\n",
    "                         main_icd9_pd['eid'],\n",
    "                         interview_pd['eid'], #Include if using verbal interview as filter\n",
    "                         firstocc_pd['eid'] #Include if using first occurrence as filter\n",
    "                        ]).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c79e2c19-2582-4a0e-af78-b8ca6ce79453",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['3999490', '5935029', '2986828', ..., '5946661', '5988229',\n",
       "       '6025513'], dtype=object)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overlap_eid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5ca0ac01-6eb7-463b-b313-58b032a6b485",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# pd.DataFrame(overlap_eid).to_csv('ukb_rap_HF_20240209.csv', index=False)\n",
    "#pd.DataFrame(overlap_eid).to_csv('ukb_rap_HCM_20240209.csv', index=False\n",
    "pd.DataFrame(overlap_eid).to_csv('ukb_rap_T2D_20240209.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6f1a70-43ca-4e82-969b-9c344e2b5dd9",
   "metadata": {},
   "source": [
    "# Filter for Individuals with Self-Reported Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b9220f-c49b-4066-a936-a6d8a63368f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter within Interview/Self-Reported for Operations\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "from functools import reduce\n",
    "#Also filter for individuals with  via verbal interview\n",
    "# Step 1: Identify columns starting with 'p2002'\n",
    "p2004_cols = [col for col in main_df.columns if col.startswith('p20004')]\n",
    "\n",
    "# Step 2: Create condition for each column and change here too if you want to filter for other diseases\n",
    "\n",
    "#For ICD\n",
    "#conditions = [F.col(col).contains('defibrillator/icd insertion') for col in p2004_cols]\n",
    "\n",
    "#For Heart Transplant\n",
    "conditions = [F.col(col).contains('heart transplant') for col in p2004_cols]\n",
    "\n",
    "# Step 3: Combine all conditions\n",
    "all_conditions = reduce(lambda a, b: a | b, conditions)\n",
    "\n",
    "# Step 4: Apply the filter\n",
    "interview_operations = main_df.filter(all_conditions)\n",
    "interview_operations.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2730f8-fa37-4366-8a22-19545da30753",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export the IDs of those who have either self-reported ICD implantation or Heart Transplant\n",
    "import pandas as pd\n",
    "interview_operations_pd = interview_operations.toPandas()\n",
    "# pd.DataFrame(interview_operations_pd['eid'].unique()).to_csv('ukb_rap_SelfReportICD_20240209.csv', index=False)\n",
    "pd.DataFrame(interview_operations_pd['eid'].unique()).to_csv('ukb_rap_SelfReportHeartTransplant_20240209.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4eba7abb-e406-402c-b7a3-dca220de4cb9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "eid    141\n",
       "dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extract the fields corresponding to the operation code and the interpolated year of operation from self-report but only for these IDs\n",
    "\n",
    "opids_ht = pd.read_csv('ukb_rap_SelfReportHeartTransplant_20240209.csv')\n",
    "opids_icd = pd.read_csv('ukb_rap_SelfReportICD_20240209.csv')\n",
    "\n",
    "opids=pd.concat([opids_ht, opids_icd], axis=0)\n",
    "opids.rename(columns={\"0\": \"eid\"}, inplace=True)\n",
    "\n",
    "opids.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9b22db04-5c74-4d1b-a686-00a61f25276c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Retrieve the Spark DataFrame corresponding to the fields of interest\n",
    "opfieldnames = ['eid'] + operations_selfreport + operations_selfreport_dates\n",
    "selfreported_op_df = participant.retrieve_fields(names=opfieldnames, engine=dxdata.connect(), coding_values='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a162bd41-5a61-461b-8499-b54a05c347b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(eid='3799870', p20004_i0_a0=None, p20004_i0_a1=None, p20004_i0_a2=None, p20004_i0_a3=None, p20004_i0_a4=None, p20004_i0_a5=None, p20004_i0_a6=None, p20004_i0_a7=None, p20004_i0_a8=None, p20004_i0_a9=None, p20004_i0_a10=None, p20004_i0_a11=None, p20004_i0_a12=None, p20004_i0_a13=None, p20004_i0_a14=None, p20004_i0_a15=None, p20004_i0_a16=None, p20004_i0_a17=None, p20004_i0_a18=None, p20004_i0_a19=None, p20004_i0_a20=None, p20004_i0_a21=None, p20004_i0_a22=None, p20004_i0_a23=None, p20004_i0_a24=None, p20004_i0_a25=None, p20004_i0_a26=None, p20004_i0_a27=None, p20004_i0_a28=None, p20004_i0_a29=None, p20004_i0_a30=None, p20004_i0_a31=None, p20004_i1_a0=None, p20004_i1_a1=None, p20004_i1_a2=None, p20004_i1_a3=None, p20004_i1_a4=None, p20004_i1_a5=None, p20004_i1_a6=None, p20004_i1_a7=None, p20004_i1_a8=None, p20004_i1_a9=None, p20004_i1_a10=None, p20004_i1_a11=None, p20004_i1_a12=None, p20004_i1_a13=None, p20004_i1_a14=None, p20004_i1_a15=None, p20004_i1_a16=None, p20004_i1_a17=None, p20004_i1_a18=None, p20004_i1_a19=None, p20004_i1_a20=None, p20004_i1_a21=None, p20004_i1_a22=None, p20004_i1_a23=None, p20004_i1_a24=None, p20004_i1_a25=None, p20004_i1_a26=None, p20004_i1_a27=None, p20004_i1_a28=None, p20004_i1_a29=None, p20004_i1_a30=None, p20004_i1_a31=None, p20004_i2_a0=None, p20004_i2_a1=None, p20004_i2_a2=None, p20004_i2_a3=None, p20004_i2_a4=None, p20004_i2_a5=None, p20004_i2_a6=None, p20004_i2_a7=None, p20004_i2_a8=None, p20004_i2_a9=None, p20004_i2_a10=None, p20004_i2_a11=None, p20004_i2_a12=None, p20004_i2_a13=None, p20004_i2_a14=None, p20004_i2_a15=None, p20004_i2_a16=None, p20004_i2_a17=None, p20004_i2_a18=None, p20004_i2_a19=None, p20004_i2_a20=None, p20004_i2_a21=None, p20004_i2_a22=None, p20004_i2_a23=None, p20004_i2_a24=None, p20004_i2_a25=None, p20004_i2_a26=None, p20004_i2_a27=None, p20004_i2_a28=None, p20004_i2_a29=None, p20004_i2_a30=None, p20004_i2_a31=None, p20004_i3_a0=None, p20004_i3_a1=None, p20004_i3_a2=None, p20004_i3_a3=None, p20004_i3_a4=None, p20004_i3_a5=None, p20004_i3_a6=None, p20004_i3_a7=None, p20004_i3_a8=None, p20004_i3_a9=None, p20004_i3_a10=None, p20004_i3_a11=None, p20004_i3_a12=None, p20004_i3_a13=None, p20004_i3_a14=None, p20004_i3_a15=None, p20004_i3_a16=None, p20004_i3_a17=None, p20004_i3_a18=None, p20004_i3_a19=None, p20004_i3_a20=None, p20004_i3_a21=None, p20004_i3_a22=None, p20004_i3_a23=None, p20004_i3_a24=None, p20004_i3_a25=None, p20004_i3_a26=None, p20004_i3_a27=None, p20004_i3_a28=None, p20004_i3_a29=None, p20004_i3_a30=None, p20004_i3_a31=None, p20010_i0_a0=None, p20010_i0_a1=None, p20010_i0_a2=None, p20010_i0_a3=None, p20010_i0_a4=None, p20010_i0_a5=None, p20010_i0_a6=None, p20010_i0_a7=None, p20010_i0_a8=None, p20010_i0_a9=None, p20010_i0_a10=None, p20010_i0_a11=None, p20010_i0_a12=None, p20010_i0_a13=None, p20010_i0_a14=None, p20010_i0_a15=None, p20010_i0_a16=None, p20010_i0_a17=None, p20010_i0_a18=None, p20010_i0_a19=None, p20010_i0_a20=None, p20010_i0_a21=None, p20010_i0_a22=None, p20010_i0_a23=None, p20010_i0_a24=None, p20010_i0_a25=None, p20010_i0_a26=None, p20010_i0_a27=None, p20010_i0_a28=None, p20010_i0_a29=None, p20010_i0_a30=None, p20010_i0_a31=None, p20010_i1_a0=None, p20010_i1_a1=None, p20010_i1_a2=None, p20010_i1_a3=None, p20010_i1_a4=None, p20010_i1_a5=None, p20010_i1_a6=None, p20010_i1_a7=None, p20010_i1_a8=None, p20010_i1_a9=None, p20010_i1_a10=None, p20010_i1_a11=None, p20010_i1_a12=None, p20010_i1_a13=None, p20010_i1_a14=None, p20010_i1_a15=None, p20010_i1_a16=None, p20010_i1_a17=None, p20010_i1_a18=None, p20010_i1_a19=None, p20010_i1_a20=None, p20010_i1_a21=None, p20010_i1_a22=None, p20010_i1_a23=None, p20010_i1_a24=None, p20010_i1_a25=None, p20010_i1_a26=None, p20010_i1_a27=None, p20010_i1_a28=None, p20010_i1_a29=None, p20010_i1_a30=None, p20010_i1_a31=None, p20010_i2_a0=None, p20010_i2_a1=None, p20010_i2_a2=None, p20010_i2_a3=None, p20010_i2_a4=None, p20010_i2_a5=None, p20010_i2_a6=None, p20010_i2_a7=None, p20010_i2_a8=None, p20010_i2_a9=None, p20010_i2_a10=None, p20010_i2_a11=None, p20010_i2_a12=None, p20010_i2_a13=None, p20010_i2_a14=None, p20010_i2_a15=None, p20010_i2_a16=None, p20010_i2_a17=None, p20010_i2_a18=None, p20010_i2_a19=None, p20010_i2_a20=None, p20010_i2_a21=None, p20010_i2_a22=None, p20010_i2_a23=None, p20010_i2_a24=None, p20010_i2_a25=None, p20010_i2_a26=None, p20010_i2_a27=None, p20010_i2_a28=None, p20010_i2_a29=None, p20010_i2_a30=None, p20010_i2_a31=None, p20010_i3_a0=None, p20010_i3_a1=None, p20010_i3_a2=None, p20010_i3_a3=None, p20010_i3_a4=None, p20010_i3_a5=None, p20010_i3_a6=None, p20010_i3_a7=None, p20010_i3_a8=None, p20010_i3_a9=None, p20010_i3_a10=None, p20010_i3_a11=None, p20010_i3_a12=None, p20010_i3_a13=None, p20010_i3_a14=None, p20010_i3_a15=None, p20010_i3_a16=None, p20010_i3_a17=None, p20010_i3_a18=None, p20010_i3_a19=None, p20010_i3_a20=None, p20010_i3_a21=None, p20010_i3_a22=None, p20010_i3_a23=None, p20010_i3_a24=None, p20010_i3_a25=None, p20010_i3_a26=None, p20010_i3_a27=None, p20010_i3_a28=None, p20010_i3_a29=None, p20010_i3_a30=None, p20010_i3_a31=None)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selfreported_op_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "87ec7a6d-210a-40b0-b4f6-6691b5a05267",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/cluster/spark/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "output_op_df = selfreported_op_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8ead0218-6bce-43ca-9eb3-fabd9531a397",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(502209, 257)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_op_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0529346c-edf8-4dbb-b6a9-33ce045c058b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Filter for only individuals in opids\n",
    "filtered_op_df=output_op_df[output_op_df['eid'].astype('int64').isin(opids['eid'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3b888e71-f9dc-4ae4-9d9a-2cff2fcb3c90",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eid</th>\n",
       "      <th>p20004_i0_a0</th>\n",
       "      <th>p20004_i0_a1</th>\n",
       "      <th>p20004_i0_a2</th>\n",
       "      <th>p20004_i0_a3</th>\n",
       "      <th>p20004_i0_a4</th>\n",
       "      <th>p20004_i0_a5</th>\n",
       "      <th>p20004_i0_a6</th>\n",
       "      <th>p20004_i0_a7</th>\n",
       "      <th>p20004_i0_a8</th>\n",
       "      <th>...</th>\n",
       "      <th>p20010_i3_a22</th>\n",
       "      <th>p20010_i3_a23</th>\n",
       "      <th>p20010_i3_a24</th>\n",
       "      <th>p20010_i3_a25</th>\n",
       "      <th>p20010_i3_a26</th>\n",
       "      <th>p20010_i3_a27</th>\n",
       "      <th>p20010_i3_a28</th>\n",
       "      <th>p20010_i3_a29</th>\n",
       "      <th>p20010_i3_a30</th>\n",
       "      <th>p20010_i3_a31</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6005</th>\n",
       "      <td>5979675</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11501</th>\n",
       "      <td>5280005</td>\n",
       "      <td>inguinal/femoral hernia repair</td>\n",
       "      <td>pacemaker/defibrillator insertion</td>\n",
       "      <td>heart transplant</td>\n",
       "      <td>tonsillectomy +/- adenoids</td>\n",
       "      <td>wisdom teeth surgery</td>\n",
       "      <td>vasectomy</td>\n",
       "      <td>coronary angiogram</td>\n",
       "      <td>colonoscopy/sigmoidoscopy</td>\n",
       "      <td>endoscopy / gastroscopy</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12662</th>\n",
       "      <td>3790249</td>\n",
       "      <td>urethral surgery</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13313</th>\n",
       "      <td>5283448</td>\n",
       "      <td>defibrillator/icd insertion</td>\n",
       "      <td>appendicectomy</td>\n",
       "      <td>ectopic pregnancy surgery</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25543</th>\n",
       "      <td>1708649</td>\n",
       "      <td>defibrillator/icd insertion</td>\n",
       "      <td>tonsillectomy / tonsil surgery</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 257 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           eid                    p20004_i0_a0  \\\n",
       "6005   5979675                            None   \n",
       "11501  5280005  inguinal/femoral hernia repair   \n",
       "12662  3790249                urethral surgery   \n",
       "13313  5283448     defibrillator/icd insertion   \n",
       "25543  1708649     defibrillator/icd insertion   \n",
       "\n",
       "                            p20004_i0_a1               p20004_i0_a2  \\\n",
       "6005                                None                       None   \n",
       "11501  pacemaker/defibrillator insertion           heart transplant   \n",
       "12662                               None                       None   \n",
       "13313                     appendicectomy  ectopic pregnancy surgery   \n",
       "25543     tonsillectomy / tonsil surgery                       None   \n",
       "\n",
       "                     p20004_i0_a3          p20004_i0_a4 p20004_i0_a5  \\\n",
       "6005                         None                  None         None   \n",
       "11501  tonsillectomy +/- adenoids  wisdom teeth surgery    vasectomy   \n",
       "12662                        None                  None         None   \n",
       "13313                        None                  None         None   \n",
       "25543                        None                  None         None   \n",
       "\n",
       "             p20004_i0_a6               p20004_i0_a7             p20004_i0_a8  \\\n",
       "6005                 None                       None                     None   \n",
       "11501  coronary angiogram  colonoscopy/sigmoidoscopy  endoscopy / gastroscopy   \n",
       "12662                None                       None                     None   \n",
       "13313                None                       None                     None   \n",
       "25543                None                       None                     None   \n",
       "\n",
       "       ... p20010_i3_a22 p20010_i3_a23 p20010_i3_a24 p20010_i3_a25  \\\n",
       "6005   ...          None          None          None          None   \n",
       "11501  ...          None          None          None          None   \n",
       "12662  ...          None          None          None          None   \n",
       "13313  ...          None          None          None          None   \n",
       "25543  ...          None          None          None          None   \n",
       "\n",
       "      p20010_i3_a26 p20010_i3_a27 p20010_i3_a28 p20010_i3_a29 p20010_i3_a30  \\\n",
       "6005           None          None          None          None          None   \n",
       "11501          None          None          None          None          None   \n",
       "12662          None          None          None          None          None   \n",
       "13313          None          None          None          None          None   \n",
       "25543          None          None          None          None          None   \n",
       "\n",
       "      p20010_i3_a31  \n",
       "6005           None  \n",
       "11501          None  \n",
       "12662          None  \n",
       "13313          None  \n",
       "25543          None  \n",
       "\n",
       "[5 rows x 257 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_op_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ecef3243-574a-4b57-bb1a-de80ac571597",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filtered_op_df.to_csv('selfreported_opICD_HT_filtered.tsv', sep='\\t', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38565f17-4930-4f52-9723-4e168d91fa4c",
   "metadata": {},
   "source": [
    "# Extract Diagnoses (Self-Reported/ICD10/9) and Date of Diagnoses (Self-Reported/ICD10/9)\n",
    "\n",
    "This further adds the dates of diagnoses either via using the self-reported interpolated year; the date of diagnosis from hospital diagnoses (for HCM)\n",
    "OR\n",
    "the date of first occurrence for HF (I50) and T2D (E11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e4a6b832-6a70-4d75-b084-4d86faf1a5bd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_82/339987039.py:7: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  return sorted(fields, key=lambda f: LooseVersion(f.name))\n"
     ]
    }
   ],
   "source": [
    "#Retrieve the Spark DataFrame corresponding to the fields of interest\n",
    "\n",
    "non_cancer_illness_fields = sum([field_names_for_id(20002)],[]) \n",
    "non_cancer_illness_fields_interpolatedyear = sum([field_names_for_id(20008)],[]) \n",
    "\n",
    "icd10_dates = sum([field_names_for_id(41280)],[]) \n",
    "icd9_dates = sum([field_names_for_id(41281)],[]) \n",
    "\n",
    "fieldnames = ['eid', 'p41270','p41271'] + icd10_dates + icd9_dates + non_cancer_illness_fields + non_cancer_illness_fields_interpolatedyear + first_hf_occurrence + first_t2d_occurrence\n",
    "selfrep_hospital_diagnoses = participant.retrieve_fields(names=fieldnames, engine=dxdata.connect(), coding_values='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7014c4f8-544d-4371-808b-80d33c6b5926",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Read in the HF and the HCM case IDs\n",
    "import pandas as pd\n",
    "hf_ids = pd.read_csv('ukb_rap_HF_20240209.csv')\n",
    "hcm_ids = pd.read_csv('ukb_rap_HCM_20240209.csv')\n",
    "t2d_ids = pd.read_csv('ukb_rap_T2D_20240209.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ae28d647-900d-47cd-af4f-293c6750715e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Filter the Spark dataframe for rows corresponding to either HF cases or HCM cases (all cases from ICD10/9; self-reported; death cause record)\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "hf_selfrep_hospital_diagnoses_df = selfrep_hospital_diagnoses.filter(col('eid').isin(hf_ids.iloc[:,0].tolist()))\n",
    "hcm_selfrep_hospital_diagnoses_df = selfrep_hospital_diagnoses.filter(col('eid').isin(hcm_ids.iloc[:,0].tolist()))\n",
    "t2d_selfrep_hospital_diagnoses_df = selfrep_hospital_diagnoses.filter(col('eid').isin(t2d_ids.iloc[:,0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "face3efc-23a4-4bb1-a7a2-d3ba5155b2cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "hcm_selfrep_hospital_diagnoses_pdf = hcm_selfrep_hospital_diagnoses_df.toPandas()\n",
    "hf_selfrep_hospital_diagnoses_pdf = hf_selfrep_hospital_diagnoses_df.toPandas()\n",
    "t2d_selfrep_hospital_diagnoses_pdf = t2d_selfrep_hospital_diagnoses_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9a604068-9228-4eca-acb9-6a99235f2e43",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eid</th>\n",
       "      <th>p41270</th>\n",
       "      <th>p41271</th>\n",
       "      <th>p41280_a0</th>\n",
       "      <th>p41280_a1</th>\n",
       "      <th>p41280_a2</th>\n",
       "      <th>p41280_a3</th>\n",
       "      <th>p41280_a4</th>\n",
       "      <th>p41280_a5</th>\n",
       "      <th>p41280_a6</th>\n",
       "      <th>...</th>\n",
       "      <th>p20008_i3_a26</th>\n",
       "      <th>p20008_i3_a27</th>\n",
       "      <th>p20008_i3_a28</th>\n",
       "      <th>p20008_i3_a29</th>\n",
       "      <th>p20008_i3_a30</th>\n",
       "      <th>p20008_i3_a31</th>\n",
       "      <th>p20008_i3_a32</th>\n",
       "      <th>p20008_i3_a33</th>\n",
       "      <th>p131354</th>\n",
       "      <th>p130708</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1026433</td>\n",
       "      <td>[C61 Malignant neoplasm of prostate, D69.6 Thr...</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-02-11</td>\n",
       "      <td>2017-12-09</td>\n",
       "      <td>2021-12-11</td>\n",
       "      <td>2017-12-09</td>\n",
       "      <td>2021-12-11</td>\n",
       "      <td>2021-12-11</td>\n",
       "      <td>2021-04-19</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1032235</td>\n",
       "      <td>[E02 Subclinical iodine-deficiency hypothyroid...</td>\n",
       "      <td>None</td>\n",
       "      <td>2017-08-15</td>\n",
       "      <td>2004-10-28</td>\n",
       "      <td>2018-01-16</td>\n",
       "      <td>2017-11-28</td>\n",
       "      <td>2018-01-16</td>\n",
       "      <td>2017-11-07</td>\n",
       "      <td>2018-11-15</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2016-05-09</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1038362</td>\n",
       "      <td>[G47.3 Sleep apnoea, I10 Essential (primary) h...</td>\n",
       "      <td>None</td>\n",
       "      <td>2013-10-16</td>\n",
       "      <td>2003-04-30</td>\n",
       "      <td>2003-04-30</td>\n",
       "      <td>2013-01-22</td>\n",
       "      <td>2013-10-16</td>\n",
       "      <td>1999-02-15</td>\n",
       "      <td>1998-12-16</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1053883</td>\n",
       "      <td>[A09.9 Gastroenteritis and colitis of unspecif...</td>\n",
       "      <td>None</td>\n",
       "      <td>2014-02-04</td>\n",
       "      <td>2006-08-15</td>\n",
       "      <td>1998-06-24</td>\n",
       "      <td>2017-10-20</td>\n",
       "      <td>2017-10-20</td>\n",
       "      <td>2018-05-01</td>\n",
       "      <td>2004-07-05</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2004-07-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1072887</td>\n",
       "      <td>[H61.0 Perichondritis of external ear, I20.9 A...</td>\n",
       "      <td>None</td>\n",
       "      <td>1999-02-15</td>\n",
       "      <td>2020-08-20</td>\n",
       "      <td>2021-04-03</td>\n",
       "      <td>2021-04-22</td>\n",
       "      <td>2020-08-05</td>\n",
       "      <td>2020-08-20</td>\n",
       "      <td>2021-04-09</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 583 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       eid                                             p41270 p41271  \\\n",
       "0  1026433  [C61 Malignant neoplasm of prostate, D69.6 Thr...   None   \n",
       "1  1032235  [E02 Subclinical iodine-deficiency hypothyroid...   None   \n",
       "2  1038362  [G47.3 Sleep apnoea, I10 Essential (primary) h...   None   \n",
       "3  1053883  [A09.9 Gastroenteritis and colitis of unspecif...   None   \n",
       "4  1072887  [H61.0 Perichondritis of external ear, I20.9 A...   None   \n",
       "\n",
       "    p41280_a0   p41280_a1   p41280_a2   p41280_a3   p41280_a4   p41280_a5  \\\n",
       "0  2021-02-11  2017-12-09  2021-12-11  2017-12-09  2021-12-11  2021-12-11   \n",
       "1  2017-08-15  2004-10-28  2018-01-16  2017-11-28  2018-01-16  2017-11-07   \n",
       "2  2013-10-16  2003-04-30  2003-04-30  2013-01-22  2013-10-16  1999-02-15   \n",
       "3  2014-02-04  2006-08-15  1998-06-24  2017-10-20  2017-10-20  2018-05-01   \n",
       "4  1999-02-15  2020-08-20  2021-04-03  2021-04-22  2020-08-05  2020-08-20   \n",
       "\n",
       "    p41280_a6  ... p20008_i3_a26 p20008_i3_a27 p20008_i3_a28 p20008_i3_a29  \\\n",
       "0  2021-04-19  ...          None          None          None          None   \n",
       "1  2018-11-15  ...          None          None          None          None   \n",
       "2  1998-12-16  ...          None          None          None          None   \n",
       "3  2004-07-05  ...          None          None          None          None   \n",
       "4  2021-04-09  ...          None          None          None          None   \n",
       "\n",
       "  p20008_i3_a30 p20008_i3_a31 p20008_i3_a32 p20008_i3_a33     p131354  \\\n",
       "0          None          None          None          None        None   \n",
       "1          None          None          None          None  2016-05-09   \n",
       "2          None          None          None          None        None   \n",
       "3          None          None          None          None        None   \n",
       "4          None          None          None          None        None   \n",
       "\n",
       "      p130708  \n",
       "0        None  \n",
       "1        None  \n",
       "2        None  \n",
       "3  2004-07-05  \n",
       "4        None  \n",
       "\n",
       "[5 rows x 583 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hcm_selfrep_hospital_diagnoses_pdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6d879a4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hcm_selfrep_hospital_diagnoses_pdf.to_csv('ukb_HCM_20240209_icd10_icd9_selfrep_diagdates.tsv', sep='\\t', index=False)\n",
    "hf_selfrep_hospital_diagnoses_pdf.to_csv('ukb_HF_20240209_icd10_icd9_selfrep_diagdates.tsv', sep='\\t', index=False)\n",
    "t2d_selfrep_hospital_diagnoses_pdf.to_csv('ukb_T2D_20240209_icd10_icd9_selfrep_diagdates.tsv', sep='\\t', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
